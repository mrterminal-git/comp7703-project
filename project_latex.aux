\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Aims}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Data}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Data split}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Data Imbalance}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Data Standardisation}{4}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Distribution of the data before standardization.}}{5}{}\protected@file@percent }
\newlabel{fig:data_distribution}{{1}{5}{}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Principle Component Analysis}{5}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Cumulative variance explained by each principal component.}}{6}{}\protected@file@percent }
\newlabel{fig:scree_plot}{{2}{6}{}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Correlation matrix of the standardized data.}}{7}{}\protected@file@percent }
\newlabel{fig:correlation_matrix}{{3}{7}{}{figure.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}K-NN Classifier}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Accuracy of K-NN Classifier}{8}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Accuracy of K-NN classifier against the train and test data.}}{8}{}\protected@file@percent }
\newlabel{fig:knn_accuracy}{{4}{8}{}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Accuracy of K-NN classifier on PCA-reduced data against the test data.}}{9}{}\protected@file@percent }
\newlabel{fig:knn_accuracy_pca}{{5}{9}{}{figure.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}K-Fold Cross-validation}{9}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Accuracy of K-NN classifier with K-Fold Cross-validation. Each box and whisker plots shows the spread of the accuracy against the test data across the 10 folds. The red line shows the mean accuracy across the 10 folds.}}{10}{}\protected@file@percent }
\newlabel{fig:knn_accuracy_kfold}{{6}{10}{}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}ROC Curve}{10}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces ROC curve for K-NN classifier with k=1.}}{11}{}\protected@file@percent }
\newlabel{fig:knn_roc_curve}{{7}{11}{}{figure.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Confusion Matrix}{11}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Confusion matrix for K-NN classifier with k=1. (Left) Confusion matrix against the raw validation set. (Right) Confusion matrix normalized by row.}}{12}{}\protected@file@percent }
\newlabel{fig:knn_confusion_matrix}{{8}{12}{}{figure.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Precision, Recall, and F1 score}{12}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Precision, Recall, and F1 Score for each class (Validation Set)}}{12}{}\protected@file@percent }
\newlabel{tab:knn_prf1}{{1}{12}{}{table.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Random Forest Classifier}{13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Accuracy of Random Forest Classifier}{13}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Accuracy of Random Forest classifier against the train and test data.}}{13}{}\protected@file@percent }
\newlabel{fig:rf_accuracy}{{9}{13}{}{figure.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1}Computational time after dimensionality reduction}{14}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Accuracy of Random Forest classifier on PCA-reduced data. The model parameters were set to 200 trees and a maximum depth of 30.}}{14}{}\protected@file@percent }
\newlabel{fig:rf_accuracy_pca}{{10}{14}{}{figure.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}K-Fold Cross-validation}{15}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Accuracy of Random Forest classifier with K-Fold Cross-validation. Each box and whisker plots shows the spread of the accuracy against the test data across the 10 folds. The red line shows the mean accuracy across the 10 folds.}}{15}{}\protected@file@percent }
\newlabel{fig:rf_accuracy_kfold}{{11}{15}{}{figure.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}ROC curve}{16}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces ROC curve for Random Forest classifier with 100 trees and maximum depth of None.}}{16}{}\protected@file@percent }
\newlabel{fig:rf_roc_curve}{{12}{16}{}{figure.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Confusion matrix}{17}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Confusion matrix for Random Forest classifier with 100 trees and maximum depth of None. (Left) Confusion matrix against the raw validation set. (Right) Confusion matrix normalized by row.}}{17}{}\protected@file@percent }
\newlabel{fig:rf_confusion_matrix}{{13}{17}{}{figure.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Precision, Recall, and F1 score}{17}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Precision, recall, and F1-score for each class on the validation set.}}{17}{}\protected@file@percent }
\newlabel{tab:rf_classification_report}{{2}{17}{}{table.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Deep Neural Networks}{18}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Architecture of the DNN with two hidden layers. The input layer has 34 nodes. There are two hidden layers with a variable number of nodes. The output layer has 3 nodes, corresponding to the three height categories.}}{18}{}\protected@file@percent }
\newlabel{fig:dnn_architecture}{{14}{18}{}{figure.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Training}{18}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.1}Effect of hidden layer sizes}{19}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Different hidden layer sizes (h1-h2) with 5-fold cross-validation. Each box and whisker plots shows the spread of the accuracy against the test data across the 5 folds. The red line shows the mean accuracy across the 5 folds.}}{19}{}\protected@file@percent }
\newlabel{fig:dnn_training}{{15}{19}{}{figure.15}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.2}Training 192-192 DNN model}{19}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Train/test curve for the DNN with 192-192 hidden layers. The blue line shows the training accuracy, and the orange line shows the testing accuracy. The learning rate was lowered whenever the testing accuracy did not increase for 20 epochs. The model was trained for a total of 116 epochs.}}{20}{}\protected@file@percent }
\newlabel{fig:dnn_train_test_curve}{{16}{20}{}{figure.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}ROC Curve}{20}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces ROC curve for DNN with 192-192 hidden layers.}}{20}{}\protected@file@percent }
\newlabel{fig:dnn_roc_curve}{{17}{20}{}{figure.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Confusion Matrix}{21}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Confusion matrix for DNN with 192-192 hidden layers. (Left) Confusion matrix against the raw validation set. (Right) Confusion matrix normalized by row.}}{21}{}\protected@file@percent }
\newlabel{fig:dnn_confusion_matrix}{{18}{21}{}{figure.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Precision, Recall, and F1 score}{21}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Precision, Recall, and F1-score for each class on the validation set using the trained DNN model.}}{21}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusion}{22}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Appendix}{23}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}All Project Code}{23}{}\protected@file@percent }
\newlabel{lst:all_code}{{1}{23}{}{lstlisting.1}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {1}All project code}{23}{}\protected@file@percent }
\ttl@finishall
\gdef \@abspage@last{54}
